/*
 * Copyright 2024 Hypermode Inc.
 * Licensed under the terms of the Apache License, Version 2.0
 * See the LICENSE file that accompanied this code for further details.
 *
 * SPDX-FileCopyrightText: 2024 Hypermode Inc. <hello@hypermode.com>
 * SPDX-License-Identifier: Apache-2.0
 */

package openai

import (
	"bytes"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"strings"
	"time"

	"github.com/hypermodeinc/modus/sdk/go/pkg/models"
	"github.com/hypermodeinc/modus/sdk/go/pkg/utils"
	"github.com/tidwall/gjson"
	"github.com/tidwall/sjson"
)

// Provides input and output types that conform to the OpenAI Chat API,
// as described in the [API Reference] docs.
//
// [API Reference]: https://platform.openai.com/docs/api-reference/chat
type ChatModel struct {
	chatModelBase
}

type chatModelBase = models.ModelBase[ChatModelInput, ChatModelOutput]

// The input object for the OpenAI Chat API.
type ChatModelInput struct {

	// The name of the model to use for the chat.
	//
	// Must be the exact string expected by the model provider.
	// For example, "gpt-3.5-turbo".
	Model string `json:"model"`

	// The list of messages to send to the chat model.
	Messages []RequestMessage `json:"messages"`

	// Output types that you want the model to generate.
	// Text modality is implied if no modalities are specified.
	Modalities []Modality `json:"modalities,omitempty"`

	// Parameters for audio output.
	// Required when audio output is requested in the modalities field.
	Audio *AudioParameters `json:"audio,omitempty"`

	// Number between -2.0 and 2.0.
	//
	// Positive values penalize new tokens based on their existing frequency in the text so far,
	// decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"`

	// Modifies the likelihood of specified tokens appearing in the completion.
	//
	// Accepts an object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,
	// but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban
	// or exclusive selection of the relevant token.
	LogitBias map[string]float64 `json:"logit_bias,omitempty"`

	// Whether to return log probabilities of the output tokens or not,
	//
	// If true, returns the log probabilities of each output token returned in the content of message.
	Logprobs bool `json:"logprobs,omitempty"`

	// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
	// each with an associated log probability. [Logprobs] must be set to true if this parameter is used.
	TopLogprobs int `json:"top_logprobs,omitempty"`

	// The maximum number of tokens to generate in the chat completion.
	//
	// The default (0) is equivalent to 4096.
	//
	// Deprecated: Use the MaxCompletionTokens parameter instead, unless the model specifically requires passing "max_tokens".
	MaxTokens int `json:"max_tokens,omitempty"`

	// The maximum number of tokens to generate in the chat completion.
	//
	// The default (0) is equivalent to 4096.
	MaxCompletionTokens int `json:"max_completion_tokens,omitempty"`

	// The number of completions to generate for each prompt.
	//
	// The default (0) is equivalent to 1.
	N int `json:"n,omitempty"`

	// Number between -2.0 and 2.0.
	//
	// Positive values penalize new tokens based on whether they appear in the text so far,
	// increasing the model's likelihood to talk about new topics.
	PresencePenalty float64 `json:"presence_penalty,omitempty"`

	// Specifies the requested format for the response.
	//  - ResponseFormatText requests a plain text string.
	//  - ResponseFormatJson requests a JSON object.
	//  - ResponseFormatJsonSchema requests a JSON object that conforms to the provided JSON schema.
	//
	// The default is ResponseFormatText.
	ResponseFormat ResponseFormat `json:"response_format"`

	// If specified, the model will make a best effort to sample deterministically,
	// such that repeated requests with the same seed and parameters should return
	// the same result.
	//
	// Determinism is not guaranteed, and you should use the SystemFingerprint response
	// parameter to monitor changes in the backend.
	//
	// The default (0) is equivalent to a random seed.
	Seed int `json:"seed,omitempty"`

	// Specifies the latency tier to use for processing the request.
	// This is relevant for customers subscribed to the scale tier service of the model hosting platform.
	//
	// - If set to 'ServiceTierAuto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
	// - If set to 'ServiceTierAuto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
	// - If set to 'ServiceTierDefault', the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
	// - When not set, the default behavior is 'ServiceTierAuto'.
	//
	// When this parameter is set, the response `serviceTier` property will indicate the service tier utilized.
	ServiceTier ServiceTier `json:"service_tier,omitempty"`

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"`

	// A number between 0.0 and 2.0 that controls the sampling temperature.
	//
	// Higher values like 0.8 will make the output more random, while lower values
	// like 0.2 will make it more focused and deterministic.
	//
	// We generally recommend altering this or TopP but not both.
	//
	// The default value is 1.0.
	Temperature float64 `json:"temperature"`

	// An alternative to sampling with temperature, called nucleus sampling, where the model
	// considers the results of the tokens with TopP probability mass.
	//
	// For example, 0.1 means only the tokens comprising the top 10% probability mass are considered.
	//
	// We generally recommend altering this or Temperature but not both.
	//
	// The default value is 1.0.
	TopP float64 `json:"top_p"`

	// A list of tools the model may call. Currently, only functions are supported as a tool.
	// Use this to provide a list of functions the model may generate JSON inputs for.
	// A max of 128 functions are supported.
	Tools []Tool `json:"tools,omitempty"`

	// Controls which (if any) tool is called by the model.
	//  - ToolChoiceNone means the model will not call any tool and instead generates a message.
	//  - ToolChoiceAuto means the model can pick between generating a message or calling one or more tools.
	//  - ToolChoiceRequired means the model must call one or more tools.
	//  - ToolChoiceFunction(name) forces the model to call a specific tool.
	//
	// The default is ToolChoiceAuto when tools are present, and ToolChoiceNone otherwise.
	ToolChoice ToolChoice `json:"tool_choice,omitempty"`

	// Whether to enable parallel function calling during tool use.
	//
	// The default is true.
	ParallelToolCalls bool `json:"parallel_tool_calls"`

	// The user ID to associate with the request, as described in the [documentation].
	// If not specified, the request will be anonymous.
	//
	// [documentation]: https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids
	User string `json:"user,omitempty"`
}

// A type that represents the modality of the chat.
type Modality string

const (
	// Text modality requests the model to respond with text.
	// This is the default if no other modality is requested.
	ModalityText Modality = "text"

	// Audio modality requests the model to respond with spoken audio.
	// The model and host must support audio output for this to work.
	// Most models that support audio require both text and audio modalities to be specified,
	// but the text will come as a transcript in the audio response.
	ModalityAudio Modality = "audio"
)

// Parameters for audio output.
// Required when audio output is requested in the modalities field.
type AudioParameters struct {

	// The voice the model should use for audio output, such as "ash" or "ballad".
	// See the model's documentation for a list of all supported voices.
	Voice string `json:"voice"`

	// The format of the audio data, such as "wav" or "mp3".
	// See the model's documentation for a list of all supported formats.
	Format string `json:"format"`
}

// Creates a new audio output parameters object.
//   - The voice parameter is the voice the model should use for audio output, such as "ash" or "ballad".
//   - The format parameter is the format of the audio data, such as "wav" or "mp3".
//
// See the model's documentation for a list of all supported voices and formats.
func NewAudioParameters(voice, format string) *AudioParameters {
	return &AudioParameters{Voice: voice, Format: format}
}

// Requests audio modality and sets the audio parameters for the input object.
//   - The voice parameter is the voice the model should use for audio output, such as "ash" or "ballad".
//   - The format parameter is the format of the audio data, such as "wav" or "mp3".
//
// See the model's documentation for a list of all supported voices and formats.
func (ci *ChatModelInput) RequestAudioOutput(voice, format string) {
	ci.Modalities = []Modality{ModalityText, ModalityAudio}
	ci.Audio = NewAudioParameters(voice, format)
}

// An object specifying which tool the model should call.
type ToolChoice struct {

	// The type of tool to call.
	Type string `json:"type"`

	// The function to call.
	Function struct {
		// The name of the function to call.
		Name string `json:"name"`
	} `json:"function,omitempty"`
}

var (
	// Directs the model to not call any tool and instead generates a message.
	ToolChoiceNone ToolChoice = ToolChoice{Type: "none"}

	// Directs the model to pick between generating a message or calling one or more tools.
	ToolChoiceAuto ToolChoice = ToolChoice{Type: "auto"}

	// Directs that the model must call one or more tools.
	ToolChoiceRequired ToolChoice = ToolChoice{Type: "required"}

	// Forces the model to call a specific tool.
	ToolChoiceFunction = func(name string) ToolChoice {
		c := ToolChoice{Type: "function"}
		c.Function.Name = name
		return c
	}
)

// Implements the json.Marshaler interface to serialize the ToolChoice object.
func (tc ToolChoice) MarshalJSON() ([]byte, error) {
	if tc.Type != "function" {
		return json.Marshal(tc.Type)
	}

	type alias ToolChoice
	return utils.JsonSerialize(alias(tc))
}

// The output object for the OpenAI Chat API.
type ChatModelOutput struct {

	// A unique identifier for the chat completion.
	Id string `json:"id"`

	// The name of the output object type returned by the API.
	// This will always be "chat.completion".
	Object string `json:"object"`

	// A list of chat completion choices. Can be more than one if n is greater than 1 in the input options.
	Choices []Choice `json:"choices"`

	// The timestamp of when the chat completion was created.
	Created time.Time `json:"created"`

	// The name of the model used to generate the chat.
	// In most cases, this will match the requested model field in the input.
	Model string `json:"model"`

	// The service tier used for processing the request.
	//
	// This field is only included if the ServiceTier parameter is specified in the request.
	ServiceTier ServiceTier `json:"service_tier"`

	// This fingerprint represents the OpenAI backend configuration that the model runs with.
	//
	// Can be used in conjunction with the Seed request parameter to understand when backend changes
	// have been made that might impact determinism.
	SystemFingerprint string `json:"system_fingerprint"`

	// The usage statistics for the request.
	Usage Usage `json:"usage"`
}

// Implements the json.Unmarshaler interface to deserialize the ChatModelOutput object.
func (o *ChatModelOutput) UnmarshalJSON(data []byte) error {
	type alias ChatModelOutput
	aux := &struct {
		*alias
		Created int64 `json:"created"`
	}{
		alias: (*alias)(o),
	}
	if err := json.Unmarshal(data, &aux); err != nil {
		return err
	}
	o.Created = time.Unix(aux.Created, 0).UTC()
	return nil
}

// An interface to any request message.
type RequestMessage interface {
	json.Marshaler

	// The role of the author of this message.
	Role() string
}

// An interface for a message content part.
type ContentPart interface {
	isContentPart()

	// The type of the content part.
	Type() string
}

// An interface for a system message content part.
type SystemContentPart interface {
	ContentPart
	isSystemMessageContentPart()
}

// An interface for a developer message content part.
type DeveloperContentPart = SystemContentPart

// An interface for a user message content part.
type UserMessageContentPart interface {
	ContentPart
	isUserMessageContentPart()
}

// An interface for an assistant message content part.
type AssistantMessageContentPart interface {
	ContentPart
	isAssistantMessageContentPart()
}

// An interface for a tool content part.
type ToolMessageContentPart interface {
	ContentPart
	isToolMessageContentPart()
}

func (*TextContentPart) isContentPart()                 {}
func (*TextContentPart) isSystemMessageContentPart()    {}
func (*TextContentPart) isUserMessageContentPart()      {}
func (*TextContentPart) isAssistantMessageContentPart() {}
func (*TextContentPart) isToolMessageContentPart()      {}

func (*ImageContentPart) isContentPart()            {}
func (*ImageContentPart) isUserMessageContentPart() {}

func (*AudioContentPart) isContentPart()            {}
func (*AudioContentPart) isUserMessageContentPart() {}

func (*RefusalContentPart) isContentPart()                 {}
func (*RefusalContentPart) isAssistantMessageContentPart() {}

// The type of this content part, in this case "text".
func (*TextContentPart) Type() string { return "text" }

// The type of this content part, in this case "image_url".
func (*ImageContentPart) Type() string { return "image_url" }

// The type of this content part, in this case "input_audio".
func (*AudioContentPart) Type() string { return "input_audio" }

// The type of this content part, in this case "refusal".
func (*RefusalContentPart) Type() string { return "refusal" }

// A text content part.
type TextContentPart struct {
	// The text string.
	Text string
}

// Implements the json.Marshaler interface to serialize the TextContentPart object.
func (m TextContentPart) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"type":"text","text":`)
	buf.Write(gjson.AppendJSONString(nil, m.Text))
	buf.WriteByte('}')
	return buf.Bytes(), nil
}

// An image content part.
type ImageContentPart struct {
	// The image information.
	Image Image
}

// Implements the json.Marshaler interface to serialize the ImageContentPart object.
func (m ImageContentPart) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"type":"image_url","image_url":`)

	if b, err := utils.JsonSerialize(m.Image); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	buf.WriteByte('}')
	return buf.Bytes(), nil
}

// An audio content part.
type AudioContentPart struct {
	// The audio information.
	Audio Audio
}

// Implements the json.Marshaler interface to serialize the AudioContentPart object.
func (m AudioContentPart) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"type":"input_audio","input_audio":`)

	if b, err := utils.JsonSerialize(m.Audio); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	buf.WriteByte('}')
	return buf.Bytes(), nil
}

// A refusal content part.
type RefusalContentPart struct {
	// The refusal message generated by the model.
	Refusal string
}

// Implements the json.Marshaler interface to serialize the RefusalContentPart object.
func (m RefusalContentPart) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"type":"refusal","refusal":`)
	buf.Write(gjson.AppendJSONString(nil, m.Refusal))
	buf.WriteByte('}')
	return buf.Bytes(), nil
}

// An image object, used to represent an image in a content part.
type Image struct {

	// The URL of the image.
	Url string `json:"url"`

	// An optional detail string for the image.
	// Can be set to "low", "high", or "auto".
	// The default is "auto".
	Detail string `json:"detail,omitempty"`
}

// An audio object, used to represent audio in a content part.
type Audio struct {

	// The raw audio data.
	Data []byte `json:"data"`

	// The format of the audio data, such as "wav" or "mp3".
	// The format must be a valid audio format supported by the model.
	Format string `json:"format"`
}

// Creates a new text content part.
func NewTextContentPart(text string) *TextContentPart {
	return &TextContentPart{Text: text}
}

// Creates a new image content part from a URL.
// The model must support image input for this to work.
// The URL will be sent directly to the model.
// The detail parameter is optional and can be set to "low", "high", or "auto".
func NewImageContentPartFromUrl(url string, detail ...string) *ImageContentPart {
	img := Image{Url: url}
	if len(detail) > 0 {
		img.Detail = detail[0]
	}
	return &ImageContentPart{Image: img}
}

// Creates a new image content part from raw image data, supplied as either a byte slice or a base64-encoded string.
// The model must support image input for this to work.
// The contentType parameter must be a valid image MIME type supported by the model, such as "image/jpeg" or "image/png".
// The detail parameter is optional and can be set to "low", "high", or "auto".
func NewImageContentPartFromData[T []byte | string](data T, contentType string, detail ...string) *ImageContentPart {

	// Unlike audio, the url needs to contain a full mime type, not just the format.
	// Thus, add the "image/" prefix if it's missing.
	if !strings.HasPrefix(contentType, "image/") {
		contentType = "image/" + contentType
	}

	var str string
	if bytes, ok := any(data).([]byte); ok {
		str = base64.StdEncoding.EncodeToString(bytes)
	} else {
		str = any(data).(string)
	}
	url := "data:" + contentType + ";base64," + str

	img := Image{Url: url}
	if len(detail) > 0 {
		img.Detail = detail[0]
	}
	return &ImageContentPart{Image: img}
}

// Creates a new audio content part from raw audio data, supplied as either a byte slice or a base64-encoded string.
// The model must support audio input for this to work.
// The format parameter must be a valid audio format supported by the model, such as "wav" or "mp3".
func NewAudioContentPartFromData[T []byte | string](data T, format string) *AudioContentPart {

	// Unlike images, the model expects just the format, not a mime type.
	// Thus, strip the "audio/" prefix if present.
	format, _ = strings.CutPrefix(format, "audio/")

	var bytes []byte
	if b64, ok := any(data).(string); ok {
		var err error
		bytes, err = base64.StdEncoding.DecodeString(b64)
		if err != nil {
			panic(fmt.Errorf("failed to decode base64 audio data: %w", err))
		}
	} else {
		bytes = any(data).([]byte)
	}

	audio := Audio{Data: bytes, Format: format}
	return &AudioContentPart{Audio: audio}
}

// Creates a new refusal content part.
func NewRefusalContentPart(refusal string) *RefusalContentPart {
	return &RefusalContentPart{Refusal: refusal}
}

// A system message.
// System messages are used to provide setup instructions to the model.
//
// Note that system and developer messages are identical in functionality,
// but the "system" role was renamed to "developer" in the OpenAI Chat API.
// Certain models may require one or the other, so use the type that matches the model's requirements.
type SystemMessage[T string | []SystemContentPart] struct {
	// The content of the message.
	Content T

	// An optional name for the participant.
	// Provides the model information to differentiate between participants of the same role.
	Name string
}

// Creates a new system message object.
//
// Note that system and developer messages are identical in functionality,
// but the "system" role was renamed to "developer" in the OpenAI Chat API.
// Certain models may require one or the other, so use the type that matches the model's requirements.
func NewSystemMessage[T string | []SystemContentPart](content T) *SystemMessage[T] {
	return &SystemMessage[T]{
		Content: content,
	}
}

// Creates a new system message object from multiple content parts.
//
// Note that system and developer messages are identical in functionality,
// but the "system" role was renamed to "developer" in the OpenAI Chat API.
// Certain models may require one or the other, so use the type that matches the model's requirements.
func NewSystemMessageFromParts(parts ...SystemContentPart) *SystemMessage[[]SystemContentPart] {
	return NewSystemMessage(parts)
}

// The role of the author of this message, in this case "system".
func (m *SystemMessage[T]) Role() string { return "system" }

// Implements the json.Marshaler interface to serialize the SystemMessage object.
func (m SystemMessage[T]) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"role":"system","content":`)

	if b, err := utils.JsonSerialize(m.Content); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	if m.Name != "" {
		buf.WriteString(`,"name":`)
		buf.Write(gjson.AppendJSONString(nil, m.Name))
	}

	buf.WriteByte('}')

	return buf.Bytes(), nil
}

// A developer message.
// Developer messages are used to provide setup instructions to the model.
//
// Note that system and developer messages are identical in functionality,
// but the "system" role was renamed to "developer" in the OpenAI Chat API.
// Certain models may require one or the other, so use the type that matches the model's requirements.
type DeveloperMessage[T string | []DeveloperContentPart] struct {

	// The content of the message.
	Content T

	// An optional name for the participant.
	// Provides the model information to differentiate between participants of the same role.
	Name string
}

// Creates a new developer message object.
//
// Note that system and developer messages are identical in functionality,
// but the "system" role was renamed to "developer" in the OpenAI Chat API.
// Certain models may require one or the other, so use the type that matches the model's requirements.
func NewDeveloperMessage[T string | []DeveloperContentPart](content T) *DeveloperMessage[T] {
	return &DeveloperMessage[T]{
		Content: content,
	}
}

// Creates a new developer message object from multiple content parts.
//
// Note that system and developer messages are identical in functionality,
// but the "system" role was renamed to "developer" in the OpenAI Chat API.
// Certain models may require one or the other, so use the type that matches the model's requirements.
func NewDeveloperMessageFromParts(parts ...DeveloperContentPart) *DeveloperMessage[[]DeveloperContentPart] {
	return NewDeveloperMessage(parts)
}

// The role of the author of this message, in this case "developer".
func (m *DeveloperMessage[T]) Role() string { return "developer" }

// Implements the json.Marshaler interface to serialize the DeveloperMessage object.
func (m DeveloperMessage[T]) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"role":"developer","content":`)

	if b, err := utils.JsonSerialize(m.Content); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	if m.Name != "" {
		buf.WriteString(`,"name":`)
		buf.Write(gjson.AppendJSONString(nil, m.Name))
	}

	buf.WriteByte('}')

	return buf.Bytes(), nil
}

// A user message object.
type UserMessage[T string | []UserMessageContentPart] struct {

	// The content of the message.
	Content T

	// An optional name for the participant.
	// Provides the model information to differentiate between participants of the same role.
	Name string
}

// Creates a new user message object.
func NewUserMessage[T string | []UserMessageContentPart](content T) *UserMessage[T] {
	return &UserMessage[T]{
		Content: content,
	}
}

// Creates a new user message object from multiple content parts.
func NewUserMessageFromParts(parts ...UserMessageContentPart) *UserMessage[[]UserMessageContentPart] {
	return NewUserMessage(parts)
}

// The role of the author of this message, in this case "user".
func (m *UserMessage[T]) Role() string { return "user" }

// Implements the json.Marshaler interface to serialize the UserMessage object.
func (m UserMessage[T]) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"role":"user","content":`)

	if b, err := utils.JsonSerialize(m.Content); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	if m.Name != "" {
		buf.WriteString(`,"name":`)
		buf.Write(gjson.AppendJSONString(nil, m.Name))
	}

	buf.WriteByte('}')

	return buf.Bytes(), nil
}

// An assistant message object, representing a message previously generated by the model.
type AssistantMessage[T string | []AssistantMessageContentPart] struct {

	// The content of the message.
	Content T

	// An optional name for the participant.
	// Provides the model information to differentiate between participants of the same role.
	Name string

	// The refusal message generated by the model, if any.
	Refusal string

	// The tool calls generated by the model, such as function calls.
	ToolCalls []ToolCall

	// Data about a previous audio response from the model.
	Audio *AudioRef
}

// Represents a reference to a previous audio response from the model.
type AudioRef struct {

	// Unique identifier for a previous audio response from the model.
	Id string `json:"id"`
}

// Creates a new assistant message object.
func NewAssistantMessage[T string | []AssistantMessageContentPart](content T) *AssistantMessage[T] {
	return &AssistantMessage[T]{
		Content: content,
	}
}

// Creates a new assistant message object from multiple content parts.
func NewAssistantMessageFromParts(parts ...AssistantMessageContentPart) *AssistantMessage[[]AssistantMessageContentPart] {
	return NewAssistantMessage(parts)
}

// Creates a new assistant message object from a completion message, so it can be used in a conversation.
func NewAssistantMessageFromCompletionMessage(cm *CompletionMessage) *AssistantMessage[string] {
	return cm.ToAssistantMessage()
}

// Converts the completion message to an assistant message, so it can be used in a conversation.
func (m *CompletionMessage) ToAssistantMessage() *AssistantMessage[string] {
	am := &AssistantMessage[string]{
		Content:   m.Content,
		Refusal:   m.Refusal,
		ToolCalls: m.ToolCalls,
	}

	if m.Audio != nil {
		am.Audio = &AudioRef{Id: m.Audio.Id}
	}

	return am
}

// The role of the author of this message, in this case "assistant".
func (m *CompletionMessage) Role() string { return "assistant" }

// The role of the author of this message, in this case "assistant".
func (m *AssistantMessage[T]) Role() string { return "assistant" }

// Implements the json.Marshaler interface to serialize the AssistantMessage object.
func (m AssistantMessage[T]) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"role":"assistant","content":`)

	if b, err := utils.JsonSerialize(m.Content); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	if m.Name != "" {
		buf.WriteString(`,"name":`)
		buf.Write(gjson.AppendJSONString(nil, m.Name))
	}

	if m.Refusal != "" {
		buf.WriteString(`,"refusal":`)
		buf.Write(gjson.AppendJSONString(nil, m.Refusal))
	}

	if len(m.ToolCalls) > 0 {
		buf.WriteString(`,"tool_calls":[`)
		for i, tc := range m.ToolCalls {
			if i > 0 {
				buf.WriteByte(',')
			}
			b, err := json.Marshal(tc)
			if err != nil {
				return nil, err
			}
			buf.Write(b)
		}
		buf.WriteByte(']')
	}

	if m.Audio != nil {
		buf.WriteString(`,"audio":`)
		b, err := json.Marshal(m.Audio)
		if err != nil {
			return nil, err
		}
		buf.Write(b)
	}

	buf.WriteByte('}')

	return buf.Bytes(), nil
}

// A tool message object.
type ToolMessage[T string | []ToolMessageContentPart] struct {

	// The content of the message.
	Content T

	// The tool call that this message is responding to.
	ToolCallId string
}

// Creates a new tool message object.
// If the content is a string, it will be passed through unaltered.
// If the content is an error object, its error message will be used as the content.
// Otherwise, the object will be JSON serialized and sent as a JSON string.
// This function will panic if the object cannot be serialized to JSON.
func NewToolMessage(content any, toolCallId string) *ToolMessage[string] {
	if content == nil {
		return &ToolMessage[string]{
			ToolCallId: toolCallId,
		}
	}

	if str, ok := content.(string); ok {
		return &ToolMessage[string]{
			Content:    str,
			ToolCallId: toolCallId,
		}
	}

	if err, ok := content.(error); ok {
		return &ToolMessage[string]{
			Content:    err.Error(),
			ToolCallId: toolCallId,
		}
	}

	if b, err := utils.JsonSerialize(content); err != nil {
		panic(err)
	} else {
		return &ToolMessage[string]{
			Content:    string(b),
			ToolCallId: toolCallId,
		}
	}
}

// Creates a new tool message object from multiple content parts.
func NewToolMessageFromParts(toolCallId string, parts ...ToolMessageContentPart) *ToolMessage[[]ToolMessageContentPart] {
	return &ToolMessage[[]ToolMessageContentPart]{
		Content:    parts,
		ToolCallId: toolCallId,
	}
}

// The role of the author of this message, in this case "tool".
func (m *ToolMessage[T]) Role() string { return "tool" }

// Implements the json.Marshaler interface to serialize the ToolMessage object.
func (m ToolMessage[T]) MarshalJSON() ([]byte, error) {
	buf := bytes.NewBufferString(`{"role":"tool","content":`)

	if b, err := utils.JsonSerialize(m.Content); err != nil {
		return nil, err
	} else {
		buf.Write(b)
	}

	buf.WriteString(`,"tool_call_id":"` + m.ToolCallId + `"}`)
	return buf.Bytes(), nil
}

// A chat completion message generated by the model.
//
// Note that a completion message is not a valid request message.
// To use a completion message in a chat, convert it to an assistant message with the ToAssistantMessage method.
type CompletionMessage struct {

	// The content of the message.
	Content string `json:"content"`

	// The refusal message generated by the model, if any.
	Refusal string `json:"refusal,omitempty"`

	// The tool calls generated by the model, such as function calls.
	ToolCalls []ToolCall `json:"tool_calls,omitempty"`

	// The audio output generated by the model, if any.
	// Used only when audio output is requested in the modalities field, and when the model and host support audio output.
	Audio *AudioOutput `json:"audio,omitempty"`
}

// An audio output object generated by the model when using audio modality.
type AudioOutput struct {

	// Unique identifier for this audio response.
	Id string `json:"id"`

	// The time at which this audio content will no longer be accessible on the server for use in multi-turn conversations.
	ExpiresAt time.Time `json:"expires_at"`

	// The raw audio data, in the format specified in the request.
	Data []byte `json:"data"`

	// Transcript of the audio generated by the model.
	Transcript string `json:"transcript"`
}

// Implements the json.Unmarshaler interface to deserialize the AudioOutput object.
func (a *AudioOutput) UnmarshalJSON(data []byte) error {
	var raw struct {
		Id         string `json:"id"`
		ExpiresAt  int64  `json:"expires_at"`
		Data       []byte `json:"data"`
		Transcript string `json:"transcript"`
	}

	if err := json.Unmarshal(data, &raw); err != nil {
		return err
	}

	a.Id = raw.Id
	a.ExpiresAt = time.Unix(raw.ExpiresAt, 0).UTC()
	a.Data = raw.Data
	a.Transcript = raw.Transcript

	return nil
}

// A tool call object that the model may generate.
type ToolCall struct {

	// The ID of the tool call.
	Id string `json:"id"`

	// The type of the tool. Currently, only `"function"` is supported.
	Type string `json:"type"`

	// The function that the model called.
	Function FunctionCall `json:"function"`
}

// A function call object that the model may generate.
type FunctionCall struct {

	// The name of the function to call.
	Name string `json:"name"`

	// The arguments to call the function with, as generated by the model in JSON format.
	Arguments string `json:"arguments"`
}

// An object specifying the format that the model must output.
type ResponseFormat struct {

	// The type of response format.
	Type string `json:"type"`

	// The JSON schema to use for the response format.
	JsonSchema utils.RawJsonString `json:"json_schema,omitempty"`
}

var (
	// Instructs the model to output the response as a plain text string.
	// This is the default response format.
	ResponseFormatText ResponseFormat = ResponseFormat{Type: "text"}

	// Instructs the model to output the response as a JSON object.
	//  - You must also instruct the model to produce JSON yourself via a system or user message.
	//  - Additionally, if you need an array you must ask for an object that wraps the array,
	//    because the model will not reliably produce arrays directly (ie., there is no "json_array" option).
	ResponseFormatJson ResponseFormat = ResponseFormat{Type: "json_object"}

	// Enables Structured Outputs which guarantees the model will match your supplied JSON schema.
	//
	// See https://platform.openai.com/docs/guides/structured-outputs
	ResponseFormatJsonSchema = func(jsonSchema string) ResponseFormat {
		return ResponseFormat{Type: "json_schema", JsonSchema: utils.RawJsonString(jsonSchema)}
	}
)

// The OpenAI service tier used to process the request.
type ServiceTier string

const (
	// The OpenAI system will utilize scale tier credits until they are exhausted.
	ServiceTierAuto ServiceTier = "auto"

	// The request will be processed using the default OpenAI service tier with a lower
	// uptime SLA and no latency guarantee.
	ServiceTierDefault ServiceTier = "default"
)

// A completion choice object returned in the response.
type Choice struct {

	// The reason the model stopped generating tokens.
	//
	// Possible values are:
	//  - "stop" if the model hit a natural stop point or a provided stop sequence
	//  - "length" if the maximum number of tokens specified in the request was reached
	//  - "content_filter" if content was omitted due to a flag from content filters
	//  - "tool_calls" if the model called a tool
	FinishReason string `json:"finish_reason"`

	// The index of the choice in the list of choices.
	Index int `json:"index"`

	// A message generated by the model.
	Message CompletionMessage `json:"message"`

	// Log probability information for the choice.
	Logprobs Logprobs `json:"logprobs"`
}

// Log probability information for a choice.
type Logprobs struct {
	// A list of message content tokens with log probability information.
	Content []LogprobsContent `json:"content"`
}

// Log probability information for a message content token.
type LogprobsContent struct {
	LogprobsContentObject

	// List of the most likely tokens and their log probability, at this token position.
	// In rare cases, there may be fewer than the number of requested TopLogprobs returned.
	TopLogprobs []LogprobsContentObject `json:"top_logprobs"`
}

// Log probability information for the most likely tokens at a given position.
type LogprobsContentObject struct {
	// The token.
	Token string `json:"token"`

	// The log probability of this token, if it is within the top 20 most likely tokens.
	// Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
	Logprob float64 `json:"logprob"`

	// A list of integers representing the UTF-8 bytes representation of the token.
	//
	// Useful in instances where characters are represented by multiple tokens and their byte
	// representations must be combined to generate the correct text representation.
	// Can be nil if there is no bytes representation for the token.
	Bytes []byte `json:"bytes"`
}

type funcParam struct {
	Name        string
	Type        string
	Description string
}

// A tool object that the model may call.
type Tool struct {
	funcParams []funcParam

	// The type of the tool. Currently, only `"function"` is supported.
	Type string `json:"type"`

	// The definition of the function.
	Function FunctionDefinition `json:"function"`
}

// Creates a new tool object for a function.
func NewToolForFunction(name, description string) Tool {
	return Tool{
		Type: "function",
		Function: FunctionDefinition{
			Name:        name,
			Description: description,
		},
	}
}

// Adds a parameter to the function used by the tool.
// Note that the type must be a valid JSON Schema type, not a Go type.
// For example, use "integer", not "int32".
func (t Tool) WithParameter(name, jsonSchemaType, description string) Tool {
	t.funcParams = append(t.funcParams, funcParam{name, jsonSchemaType, description})

	buf := bytes.NewBufferString(`{"type":"object","properties":{`)
	for i, p := range t.funcParams {
		if i > 0 {
			buf.WriteByte(',')
		}
		buf.Write(gjson.AppendJSONString(nil, p.Name))
		buf.WriteString(`:{"type":`)
		buf.Write(gjson.AppendJSONString(nil, p.Type))
		buf.WriteString(`,"description":`)
		buf.Write(gjson.AppendJSONString(nil, p.Description))
		buf.WriteByte('}')
	}
	buf.WriteString(`},"required":[`)
	for i, p := range t.funcParams {
		if i > 0 {
			buf.WriteByte(',')
		}
		buf.Write(gjson.AppendJSONString(nil, p.Name))
	}
	buf.WriteString(`],"additionalProperties":false}`)

	t.Function.Parameters = utils.RawJsonString(buf.String())
	return t
}

// Sets the JSON Schema for the parameters of the function used by the tool.
// Use this for defining complex parameters. Prefer WithParameter for adding simple parameters,
// which will generate the schema for you automatically.
func (t Tool) WithParametersSchema(jsonSchema string) Tool {
	t.Function.Parameters = utils.RawJsonString(jsonSchema)
	return t
}

// The definition of a function that can be called by the model.
type FunctionDefinition struct {
	// The name of the function to be called.
	//
	// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
	Name string `json:"name"`

	// An optional description of what the function does, used by the model to choose when and how to call the function.
	Description string `json:"description,omitempty"`

	// Whether to enable strict schema adherence when generating the function call.
	// If set to true, the model will follow the exact schema defined in the parameters field.
	//
	// The default is false.
	//
	// See https://platform.openai.com/docs/guides/function-calling
	//
	// NOTE:
	// In order to guarantee strict schema adherence, disable parallel function calls
	// by setting ParallelToolCalls to false.
	//
	// See https://platform.openai.com/docs/guides/function-calling/parallel-function-calling-and-structured-outputs
	Strict bool `json:"strict,omitempty"`

	// The parameters the functions accepts, described as a JSON Schema object.
	//
	// See https://platform.openai.com/docs/guides/function-calling
	Parameters utils.RawJsonString `json:"parameters,omitempty"`
}

// Creates an input object for the OpenAI Chat API.
func (m *ChatModel) CreateInput(messages ...RequestMessage) (*ChatModelInput, error) {
	return &ChatModelInput{
		Model:             m.Info().FullName,
		Messages:          messages,
		ResponseFormat:    ResponseFormatText,
		Temperature:       1.0,
		TopP:              1.0,
		ParallelToolCalls: true,
	}, nil
}

// Implements the json.Marshaler interface to serialize the ChatModelInput object.
func (mi *ChatModelInput) MarshalJSON() ([]byte, error) {

	type alias ChatModelInput
	b, err := utils.JsonSerialize(alias(*mi))
	if err != nil {
		return nil, err
	}

	// omit default response_format
	if t := mi.ResponseFormat.Type; t == "" || t == "text" {
		b, err = sjson.DeleteBytes(b, "response_format")
		if err != nil {
			return nil, err
		}
	}

	// omit default temperature
	if mi.Temperature == 1.0 {
		b, err = sjson.DeleteBytes(b, "temperature")
		if err != nil {
			return nil, err
		}
	}

	// omit default top_p
	if mi.TopP == 1.0 {
		b, err = sjson.DeleteBytes(b, "top_p")
		if err != nil {
			return nil, err
		}
	}

	// don't send parallel_tool_calls if tools are not present
	if len(mi.Tools) == 0 {
		b, err = sjson.DeleteBytes(b, "parallel_tool_calls")
		if err != nil {
			return nil, err
		}
	}

	// remove empty tool_choice
	if mi.ToolChoice.Type == "" {
		b, err = sjson.DeleteBytes(b, "tool_choice")
		if err != nil {
			return nil, err
		}
	}

	return b, nil
}
